{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer #TF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>¿Ya vieron que en la #AO habrá evento por el #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>muy XD que para el primer punto exiges que Yam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Apoco lees?\\n1-El primer punto es para hablar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>vos se nota que no tenes compresión lectora xD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No, literalmente Kaido y todos los piratas bes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HATE                                         COMENTARIO\n",
       "0   0.0  ¿Ya vieron que en la #AO habrá evento por el #...\n",
       "1   0.0  muy XD que para el primer punto exiges que Yam...\n",
       "2   0.0  Apoco lees?\\n1-El primer punto es para hablar ...\n",
       "3   0.0  vos se nota que no tenes compresión lectora xD...\n",
       "4   0.0  No, literalmente Kaido y todos los piratas bes..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"DATOS_UNICOS_TOTALES.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    sw = stopwords.words('spanish')\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text)\n",
    "    urls = []\n",
    "    for i in text.split():\n",
    "        if not i.startswith('http'):\n",
    "            urls.append(i)\n",
    "    text1 = ' '.join(urls)\n",
    "    cl1 = re.findall(r'[^@#&\\w+][\\w+]+',text1)\n",
    "    text2 = ' '.join(cl1)\n",
    "    cl3 = re.findall(r'[a-zA-Záéíóúñ\\s:_]+',text2)\n",
    "    text3 = ' '.join(cl3)\n",
    "    c = text3.maketrans('áéíóú','aeiou')\n",
    "    text4 = text3.translate(c)   \n",
    "    L = [palabra for palabra in text4.split() if len(palabra) >2 and palabra not in sw]\n",
    "    return ' '.join(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiando Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_limpios = []\n",
    "for comment in df['COMENTARIO']:\n",
    "    list_limpios.append(clean(str(comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>LIMPIOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>¿Ya vieron que en la #AO habrá evento por el #...</td>\n",
       "      <td>vieron habra evento enserio rifan mostrando em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>muy XD que para el primer punto exiges que Yam...</td>\n",
       "      <td>primer punto exiges yamato autoproclame hombre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Apoco lees?\\n1-El primer punto es para hablar ...</td>\n",
       "      <td>lees primer punto hablar yamato usa pronombres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>vos se nota que no tenes compresión lectora xD...</td>\n",
       "      <td>nota tenes compresion lectora diciendo hipocri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No, literalmente Kaido y todos los piratas bes...</td>\n",
       "      <td>literalmente kaido piratas bestia incluso luff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HATE                                         COMENTARIO  \\\n",
       "0   0.0  ¿Ya vieron que en la #AO habrá evento por el #...   \n",
       "1   0.0  muy XD que para el primer punto exiges que Yam...   \n",
       "2   0.0  Apoco lees?\\n1-El primer punto es para hablar ...   \n",
       "3   0.0  vos se nota que no tenes compresión lectora xD...   \n",
       "4   0.0  No, literalmente Kaido y todos los piratas bes...   \n",
       "\n",
       "                                             LIMPIOS  \n",
       "0  vieron habra evento enserio rifan mostrando em...  \n",
       "1  primer punto exiges yamato autoproclame hombre...  \n",
       "2  lees primer punto hablar yamato usa pronombres...  \n",
       "3  nota tenes compresion lectora diciendo hipocri...  \n",
       "4  literalmente kaido piratas bestia incluso luff...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LIMPIOS'] = list_limpios\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ofensivo = df[df[\"HATE\"] == 1]\n",
    "df_noOfensivo = df[df[\"HATE\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>LIMPIOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Esto es la máxima imbecilidad</td>\n",
       "      <td>maxima imbecilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>No nos engañemos. Los homosexuales bien sabemo...</td>\n",
       "      <td>engañemos homosexuales bien sabemos union pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Claro que es ilegal.\\nSi esa fuera la razón, c...</td>\n",
       "      <td>ilegal razon claro ademas inmoral socialmente ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A los curas pro MOVIMIENTO LGTBI+ deberían ser...</td>\n",
       "      <td>curas pro movimiento lgtbi deberian ser server...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A los de siempre. Hablo del Movimiento, de la ...</td>\n",
       "      <td>siempre hablo movimiento apologia lgtbi person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@13Azulcrema Y eso que cabrón? Si allá está pe...</td>\n",
       "      <td>cabron alla penado ser gay tambien vas ser hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14181</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Cuchame, soy la mierda más delulu que te puede...</td>\n",
       "      <td>mierda mas delulu puedes encontrar adicta gay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14182</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@NEXWAL999 tampoco te las harías pq eres gay</td>\n",
       "      <td>tampoco harias gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Not mi amiga diciéndome que ha ido a comer sol...</td>\n",
       "      <td>amiga diciendome ido comer sola maricones meti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14234</th>\n",
       "      <td>1.0</td>\n",
       "      <td>¿Yo dije que por un gay se destruye la familia ?</td>\n",
       "      <td>dije gay destruye familia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HATE                                         COMENTARIO  \\\n",
       "15      1.0                      Esto es la máxima imbecilidad   \n",
       "46      1.0  No nos engañemos. Los homosexuales bien sabemo...   \n",
       "52      1.0  Claro que es ilegal.\\nSi esa fuera la razón, c...   \n",
       "89      1.0  A los curas pro MOVIMIENTO LGTBI+ deberían ser...   \n",
       "90      1.0  A los de siempre. Hablo del Movimiento, de la ...   \n",
       "...     ...                                                ...   \n",
       "14157   1.0  @13Azulcrema Y eso que cabrón? Si allá está pe...   \n",
       "14181   1.0  Cuchame, soy la mierda más delulu que te puede...   \n",
       "14182   1.0       @NEXWAL999 tampoco te las harías pq eres gay   \n",
       "14203   1.0  Not mi amiga diciéndome que ha ido a comer sol...   \n",
       "14234   1.0   ¿Yo dije que por un gay se destruye la familia ?   \n",
       "\n",
       "                                                 LIMPIOS  \n",
       "15                                    maxima imbecilidad  \n",
       "46     engañemos homosexuales bien sabemos union pers...  \n",
       "52     ilegal razon claro ademas inmoral socialmente ...  \n",
       "89     curas pro movimiento lgtbi deberian ser server...  \n",
       "90     siempre hablo movimiento apologia lgtbi person...  \n",
       "...                                                  ...  \n",
       "14157  cabron alla penado ser gay tambien vas ser hom...  \n",
       "14181  mierda mas delulu puedes encontrar adicta gay ...  \n",
       "14182                                 tampoco harias gay  \n",
       "14203  amiga diciendome ido comer sola maricones meti...  \n",
       "14234                          dije gay destruye familia  \n",
       "\n",
       "[504 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ofensivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Teste (80 - 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofen_train = df_ofensivo[:403].sample(400)\n",
    "ofen_train.dropna(how = 'any',inplace=True)\n",
    "ofen_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofen_teste = df_ofensivo[400:].sample(100)\n",
    "ofen_teste.dropna(how = 'any', inplace = True)\n",
    "ofen_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfen_train = df_noOfensivo[:3000].sample(400)\n",
    "NoOfen_train.dropna(how = 'any',inplace=True)\n",
    "NoOfen_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfen_teste = df_noOfensivo[3000:5000].sample(100)\n",
    "NoOfen_teste.dropna(how = 'any', inplace = True)\n",
    "NoOfen_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 50)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos = ofen_train['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 50)\n",
    "ofen_train_tfidf = tf_idf.fit_transform(ofensivos)\n",
    "ofen_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ahora', 'asco', 'asi', 'bandera', 'callate', 'cara', 'comunidad', 'dice', 'dios', 'enfermedad', 'enfermos', 'face_with_tears_of_joy', 'familia', 'gay', 'gays', 'gente', 'hombre', 'homosexual', 'lgbt', 'madre', 'mal', 'marica', 'maricon', 'mas', 'matrimonio', 'mexico', 'mierda', 'mismo', 'movimiento', 'nauseated_face', 'organizacion', 'palabra', 'peor', 'personas', 'puede', 'pueden', 'puta', 'puto', 'putos', 'quieren', 'rainbow_flag_selector', 'ser', 'solo', 'sos', 'tan', 'tener', 'turkey', 'vas', 'ver', 'vida']\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 50)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noOfensivos = NoOfen_train['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 50)\n",
    "NoOfen_train_tfidf = tf_idf.fit_transform(noOfensivos)\n",
    "NoOfen_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos_test = ofen_teste['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 50)\n",
    "ofen_teste_tfidf = tf_idf.fit_transform(ofensivos_test)\n",
    "ofen_teste_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfensivos_test = NoOfen_teste['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 50)\n",
    "NoOfen_teste_tfidf = tf_idf.fit_transform(NoOfensivos_test)\n",
    "NoOfen_teste_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 50)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos = ofen_train['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 50)\n",
    "ofen_train_tf = tf.fit_transform(ofensivos)\n",
    "ofen_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ahora', 'asco', 'asi', 'bandera', 'callate', 'cara', 'comunidad', 'dice', 'dios', 'enfermedad', 'enfermos', 'face_with_tears_of_joy', 'familia', 'gay', 'gays', 'gente', 'hombre', 'homosexual', 'lgbt', 'madre', 'mal', 'marica', 'maricon', 'mas', 'matrimonio', 'mexico', 'mierda', 'mismo', 'movimiento', 'nauseated_face', 'organizacion', 'palabra', 'peor', 'personas', 'puede', 'pueden', 'puta', 'puto', 'putos', 'quieren', 'rainbow_flag_selector', 'ser', 'solo', 'sos', 'tan', 'tener', 'turkey', 'vas', 'ver', 'vida']\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 50)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noOfensivos = NoOfen_train['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 50)\n",
    "NoOfen_train_tf = tf.fit_transform(noOfensivos)\n",
    "NoOfen_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos_test = ofen_teste['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 50)\n",
    "ofen_teste_tf = tf.fit_transform(ofensivos_test)\n",
    "ofen_teste_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfensivos_test = NoOfen_teste['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 50)\n",
    "NoOfen_teste_tf = tf.fit_transform(NoOfensivos_test)\n",
    "NoOfen_teste_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenando data tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 50)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.concatenate((ofen_train_tfidf.toarray().tolist(), NoOfen_train_tfidf.toarray().tolist() ), axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.concatenate((ofen_train['HATE'].values, NoOfen_train['HATE'].values), axis = 0)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste = np.concatenate((ofen_teste_tfidf.toarray().tolist(),NoOfen_teste_tfidf.toarray().tolist()), axis=0)\n",
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_teste = np.concatenate((ofen_teste['HATE'].values, NoOfen_teste['HATE']), axis = 0)\n",
    "Y_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenando data TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 50)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf = np.concatenate((ofen_train_tf.toarray().tolist(), NoOfen_train_tf.toarray().tolist() ), axis=0)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_tf = np.concatenate((ofen_train['HATE'].values, NoOfen_train['HATE'].values), axis = 0)\n",
    "Y_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste_tf = np.concatenate((ofen_teste_tf.toarray().tolist(),NoOfen_teste_tf.toarray().tolist()), axis=0)\n",
    "X_teste_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_teste_tf = np.concatenate((ofen_teste['HATE'].values, NoOfen_teste['HATE'].values), axis = 0)\n",
    "Y_teste_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10000, gamma=0.001)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = svm.SVC(gamma=0.001,C=10000)\n",
    "clasificador.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clasificador.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_teste,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.88      0.74       100\n",
      "         1.0       0.81      0.50      0.62       100\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.72      0.69      0.68       200\n",
      "weighted avg       0.72      0.69      0.68       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_teste,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100000, gamma=0.001)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = svm.SVC(gamma=0.001,C=100000)\n",
    "clasificador.fit(X_train_tf,Y_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_tf = clasificador.predict(X_teste_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_teste_tf,Y_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.89      0.81       100\n",
      "         1.0       0.86      0.70      0.77       100\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.81      0.79      0.79       200\n",
      "weighted avg       0.81      0.80      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_teste_tf,Y_pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88, 12],\n",
       "       [50, 50]], dtype=int64)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_teste, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[89, 11],\n",
       "       [30, 70]], dtype=int64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_teste_tf, Y_pred_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
