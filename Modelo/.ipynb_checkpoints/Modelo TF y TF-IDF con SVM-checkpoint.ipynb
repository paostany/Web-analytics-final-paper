{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer #TF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>¿Ya vieron que en la #AO habrá evento por el #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>muy XD que para el primer punto exiges que Yam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Apoco lees?\\n1-El primer punto es para hablar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>vos se nota que no tenes compresión lectora xD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No, literalmente Kaido y todos los piratas bes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HATE                                         COMENTARIO\n",
       "0   0.0  ¿Ya vieron que en la #AO habrá evento por el #...\n",
       "1   0.0  muy XD que para el primer punto exiges que Yam...\n",
       "2   0.0  Apoco lees?\\n1-El primer punto es para hablar ...\n",
       "3   0.0  vos se nota que no tenes compresión lectora xD...\n",
       "4   0.0  No, literalmente Kaido y todos los piratas bes..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"DATOS_UNICOS_TOTALES.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('stopwords1.txt', 'r', encoding = 'utf-8')\n",
    "stopwords_1 = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    #sw = stopwords.words('spanish')\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text)\n",
    "    urls = []\n",
    "    for i in text.split():\n",
    "        if not i.startswith('http'):\n",
    "            urls.append(i)\n",
    "    text1 = ' '.join(urls)\n",
    "    cl1 = re.findall(r'[^@#&\\w+][\\w+]+',text1)\n",
    "    text2 = ' '.join(cl1)\n",
    "    cl3 = re.findall(r'[a-zA-Záéíóúñ\\s:_]+',text2)\n",
    "    text3 = ' '.join(cl3)\n",
    "    c = text3.maketrans('áéíóú','aeiou')\n",
    "    text4 = text3.translate(c)   \n",
    "    L = [palabra for palabra in text4.split() if len(palabra) >2 and palabra not in stopwords_1]\n",
    "    return ' '.join(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiando Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_limpios = []\n",
    "for comment in df['COMENTARIO']:\n",
    "    list_limpios.append(clean(str(comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>LIMPIOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>¿Ya vieron que en la #AO habrá evento por el #...</td>\n",
       "      <td>vieron que habra evento por enserio que rifan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>muy XD que para el primer punto exiges que Yam...</td>\n",
       "      <td>que para primer punto exiges que yamato autopr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Apoco lees?\\n1-El primer punto es para hablar ...</td>\n",
       "      <td>lees primer punto para hablar por que yamato u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>vos se nota que no tenes compresión lectora xD...</td>\n",
       "      <td>nota que tenes compresion lectora estoy dicien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No, literalmente Kaido y todos los piratas bes...</td>\n",
       "      <td>literalmente kaido todos los piratas bestia in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HATE                                         COMENTARIO  \\\n",
       "0   0.0  ¿Ya vieron que en la #AO habrá evento por el #...   \n",
       "1   0.0  muy XD que para el primer punto exiges que Yam...   \n",
       "2   0.0  Apoco lees?\\n1-El primer punto es para hablar ...   \n",
       "3   0.0  vos se nota que no tenes compresión lectora xD...   \n",
       "4   0.0  No, literalmente Kaido y todos los piratas bes...   \n",
       "\n",
       "                                             LIMPIOS  \n",
       "0  vieron que habra evento por enserio que rifan ...  \n",
       "1  que para primer punto exiges que yamato autopr...  \n",
       "2  lees primer punto para hablar por que yamato u...  \n",
       "3  nota que tenes compresion lectora estoy dicien...  \n",
       "4  literalmente kaido todos los piratas bestia in...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LIMPIOS'] = list_limpios\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ofensivo = df[df[\"HATE\"] == 1]\n",
    "df_noOfensivo = df[df[\"HATE\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HATE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>LIMPIOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Esto es la máxima imbecilidad</td>\n",
       "      <td>maxima imbecilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>No nos engañemos. Los homosexuales bien sabemo...</td>\n",
       "      <td>nos engañemos los homosexuales bien sabemos qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Claro que es ilegal.\\nSi esa fuera la razón, c...</td>\n",
       "      <td>que ilegal esa fuera razon claro que ademas in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A los curas pro MOVIMIENTO LGTBI+ deberían ser...</td>\n",
       "      <td>los curas pro movimiento lgtbi deberian ser se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A los de siempre. Hablo del Movimiento, de la ...</td>\n",
       "      <td>los siempre hablo del movimiento apologia lgtb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@13Azulcrema Y eso que cabrón? Si allá está pe...</td>\n",
       "      <td>eso que cabron alla esta penado ser gay tambie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14181</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Cuchame, soy la mierda más delulu que te puede...</td>\n",
       "      <td>soy mierda mas delulu que puedes encontrar soy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14182</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@NEXWAL999 tampoco te las harías pq eres gay</td>\n",
       "      <td>tampoco las harias eres gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Not mi amiga diciéndome que ha ido a comer sol...</td>\n",
       "      <td>amiga diciendome que ido comer sola que unos m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14234</th>\n",
       "      <td>1.0</td>\n",
       "      <td>¿Yo dije que por un gay se destruye la familia ?</td>\n",
       "      <td>dije que por gay destruye familia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HATE                                         COMENTARIO  \\\n",
       "15      1.0                      Esto es la máxima imbecilidad   \n",
       "46      1.0  No nos engañemos. Los homosexuales bien sabemo...   \n",
       "52      1.0  Claro que es ilegal.\\nSi esa fuera la razón, c...   \n",
       "89      1.0  A los curas pro MOVIMIENTO LGTBI+ deberían ser...   \n",
       "90      1.0  A los de siempre. Hablo del Movimiento, de la ...   \n",
       "...     ...                                                ...   \n",
       "14157   1.0  @13Azulcrema Y eso que cabrón? Si allá está pe...   \n",
       "14181   1.0  Cuchame, soy la mierda más delulu que te puede...   \n",
       "14182   1.0       @NEXWAL999 tampoco te las harías pq eres gay   \n",
       "14203   1.0  Not mi amiga diciéndome que ha ido a comer sol...   \n",
       "14234   1.0   ¿Yo dije que por un gay se destruye la familia ?   \n",
       "\n",
       "                                                 LIMPIOS  \n",
       "15                                    maxima imbecilidad  \n",
       "46     nos engañemos los homosexuales bien sabemos qu...  \n",
       "52     que ilegal esa fuera razon claro que ademas in...  \n",
       "89     los curas pro movimiento lgtbi deberian ser se...  \n",
       "90     los siempre hablo del movimiento apologia lgtb...  \n",
       "...                                                  ...  \n",
       "14157  eso que cabron alla esta penado ser gay tambie...  \n",
       "14181  soy mierda mas delulu que puedes encontrar soy...  \n",
       "14182                        tampoco las harias eres gay  \n",
       "14203  amiga diciendome que ido comer sola que unos m...  \n",
       "14234                  dije que por gay destruye familia  \n",
       "\n",
       "[504 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ofensivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Teste (80 - 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofen_train = df_ofensivo[:400].sample(400)\n",
    "ofen_train.dropna(how = 'any',inplace=True)\n",
    "ofen_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofen_teste = df_ofensivo[400:].sample(100)\n",
    "ofen_teste.dropna(how = 'any', inplace = True)\n",
    "ofen_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfen_train = df_noOfensivo[:6000].sample(400)\n",
    "NoOfen_train.dropna(how = 'any',inplace=True)\n",
    "NoOfen_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfen_teste = df_noOfensivo[6000:].sample(100)\n",
    "NoOfen_teste.dropna(how = 'any', inplace = True)\n",
    "NoOfen_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos = ofen_train['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 20)\n",
    "ofen_train_tfidf = tf_idf.fit_transform(ofensivos)\n",
    "ofen_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noOfensivos = NoOfen_train['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 20)\n",
    "NoOfen_train_tfidf = tf_idf.fit_transform(noOfensivos)\n",
    "NoOfen_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos_test = ofen_teste['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 20)\n",
    "ofen_teste_tfidf = tf_idf.fit_transform(ofensivos_test)\n",
    "ofen_teste_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfensivos_test = NoOfen_teste['LIMPIOS'].values.tolist()\n",
    "#TF-IDF\n",
    "tf_idf = TfidfVectorizer(max_features = 20)\n",
    "NoOfen_teste_tfidf = tf_idf.fit_transform(NoOfensivos_test)\n",
    "NoOfen_teste_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos = ofen_train['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 20)\n",
    "ofen_train_tf = tf.fit_transform(ofensivos)\n",
    "ofen_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 20)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noOfensivos = NoOfen_train['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 20)\n",
    "NoOfen_train_tf = tf.fit_transform(noOfensivos)\n",
    "NoOfen_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofensivos_test = ofen_teste['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 20)\n",
    "ofen_teste_tf = tf.fit_transform(ofensivos_test)\n",
    "ofen_teste_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoOfensivos_test = NoOfen_teste['LIMPIOS'].values.tolist()\n",
    "#TF\n",
    "tf = CountVectorizer(max_features = 20)\n",
    "NoOfen_teste_tf = tf.fit_transform(NoOfensivos_test)\n",
    "NoOfen_teste_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenando data tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.concatenate((ofen_train_tfidf.toarray().tolist(), NoOfen_train_tfidf.toarray().tolist() ), axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.concatenate((ofen_train['HATE'].values, NoOfen_train['HATE'].values), axis = 0)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 20)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste = np.concatenate((ofen_teste_tfidf.toarray().tolist(),NoOfen_teste_tfidf.toarray().tolist()), axis=0)\n",
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_teste = np.concatenate((ofen_teste['HATE'].values, NoOfen_teste['HATE']), axis = 0)\n",
    "Y_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenando data TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 20)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf = np.concatenate((ofen_train_tf.toarray().tolist(), NoOfen_train_tf.toarray().tolist() ), axis=0)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_tf = np.concatenate((ofen_train['HATE'].values, NoOfen_train['HATE'].values), axis = 0)\n",
    "Y_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 20)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste_tf = np.concatenate((ofen_teste_tf.toarray().tolist(),NoOfen_teste_tf.toarray().tolist()), axis=0)\n",
    "X_teste_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_teste_tf = np.concatenate((ofen_teste['HATE'].values, NoOfen_teste['HATE'].values), axis = 0)\n",
    "Y_teste_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.001)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = svm.SVC(gamma=0.001,C=10)\n",
    "clasificador.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clasificador.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_teste,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.53      0.62       100\n",
      "         1.0       0.63      0.81      0.71       100\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.68      0.67      0.66       200\n",
      "weighted avg       0.68      0.67      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_teste,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=0.01)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = svm.SVC(gamma=0.01,C=100)\n",
    "clasificador.fit(X_train_tf,Y_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_tf = clasificador.predict(X_teste_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_teste_tf,Y_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.64      0.70       100\n",
      "         1.0       0.69      0.82      0.75       100\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.74      0.73      0.73       200\n",
      "weighted avg       0.74      0.73      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_teste_tf,Y_pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69, 31],\n",
       "       [41, 59]], dtype=int64)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_teste, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[79, 21],\n",
       "       [32, 68]], dtype=int64)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_teste_tf, Y_pred_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
